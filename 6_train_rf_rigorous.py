#!/usr/bin/env python3
"""
ä¸¥è°¨ç‰ˆ Random Forest è®­ç»ƒè„šæœ¬
æ ¸å¿ƒæ”¹è¿›ï¼š
1. æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ï¼ˆTimeSeriesSplitï¼‰- é¿å…æ•°æ®æ³„æ¼
2. å…¨é¢è¯„ä¼°æŒ‡æ ‡ï¼ˆmacro F1 + confusion matrix + per-class metricsï¼‰
3. Permutation Importance - ä¿®æ­£æ ‘æ¨¡å‹åå·®
4. è¾“å‡ºæ¯å¸§æ¦‚ç‡åˆ†å¸ƒï¼ˆä¸º HMM/Viterbi å‡†å¤‡ï¼‰
"""

import argparse
import os
import json
import glob
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_predict
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.inspection import permutation_importance

# é…ç½®matplotlibæ”¯æŒä¸­æ–‡
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False


def find_latest_features_file(search_dir='datasets'):
    """è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„ç‰¹å¾CSVæ–‡ä»¶"""
    pattern = os.path.join(search_dir, '*features*.csv')
    files = glob.glob(pattern, recursive=True)
    
    if not files:
        return None
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„
    latest_file = max(files, key=os.path.getmtime)
    return latest_file


def plot_confusion_matrix_heatmap(y_true, y_pred, out_path, labels):
    """ç»˜åˆ¶å¸¦ç™¾åˆ†æ¯”å’Œç»å¯¹æ•°é‡çš„æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾"""
    cm = confusion_matrix(y_true, y_pred)
    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # ç»˜åˆ¶å½’ä¸€åŒ–çƒ­åŠ›å›¾
    sns.heatmap(cm_norm, annot=False, fmt='.2%', cmap='YlOrRd',
                xticklabels=labels, yticklabels=labels,
                cbar_kws={'label': 'æ¯”ä¾‹'}, ax=ax)
    
    # æ‰‹åŠ¨æ·»åŠ æ ‡æ³¨ï¼ˆç™¾åˆ†æ¯” + ç»å¯¹æ•°é‡ï¼‰
    for i in range(len(labels)):
        for j in range(len(labels)):
            percentage = cm_norm[i, j] * 100
            count = cm[i, j]
            text_color = 'white' if cm_norm[i, j] > 0.5 else 'black'
            ax.text(j + 0.5, i + 0.5, f'{percentage:.1f}%\n({count})',
                   ha='center', va='center', fontsize=10, color=text_color)
    
    ax.set_title('æ··æ·†çŸ©é˜µï¼ˆå½’ä¸€åŒ–ï¼‰', fontsize=14, pad=15)
    ax.set_ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
    ax.set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
    
    plt.tight_layout()
    plt.savefig(out_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"[INFO] æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾å·²ä¿å­˜: {out_path}")


def plot_per_class_metrics(report_dict, out_path, labels):
    """ç»˜åˆ¶æ¯ç±»çš„ Precisionã€Recallã€F1-Score"""
    metrics = ['precision', 'recall', 'f1-score']
    
    # æå–æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡ï¼ˆä½¿ç”¨æ ‡ç­¾åä½œä¸ºé”®ï¼‰
    data = {metric: [] for metric in metrics}
    for label in labels:
        # classification_report ä½¿ç”¨ target_names ä½œä¸ºé”®
        class_report = report_dict.get(label, {})
        for metric in metrics:
            value = class_report.get(metric, 0.0)
            data[metric].append(value)
    
    fig, ax = plt.subplots(figsize=(12, 6))
    x = np.arange(len(labels))
    width = 0.25
    
    colors = ['#3498db', '#e74c3c', '#2ecc71']
    for i, metric in enumerate(metrics):
        offset = (i - 1) * width
        bars = ax.bar(x + offset, data[metric], width, 
                     label=metric.capitalize(), color=colors[i], alpha=0.8)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.3f}', ha='center', va='bottom', fontsize=9)
    
    ax.set_ylabel('Score', fontsize=12)
    ax.set_title('Per-Class Performance Metrics', fontsize=14)
    ax.set_xticks(x)
    ax.set_xticklabels(labels)
    ax.legend(loc='lower right')
    ax.grid(axis='y', alpha=0.3, linestyle='--')
    ax.set_ylim([0, 1.1])
    
    plt.tight_layout()
    plt.savefig(out_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"[INFO] Per-class æ€§èƒ½æŒ‡æ ‡å·²ä¿å­˜: {out_path}")


def plot_permutation_importance(perm_imp, feature_names, out_path, top_n=20):
    """ç»˜åˆ¶ Permutation Importance"""
    mean_importance = perm_imp.importances_mean
    std_importance = perm_imp.importances_std
    
    # æ’åºå¹¶å–å‰ N ä¸ª
    indices = np.argsort(mean_importance)[::-1][:top_n]
    
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # ç»˜åˆ¶æ¡å½¢å›¾
    y_pos = np.arange(top_n)
    ax.barh(y_pos, mean_importance[indices], 
           xerr=std_importance[indices], color='teal', alpha=0.7,
           error_kw={'elinewidth': 1, 'capsize': 3})
    
    ax.set_yticks(y_pos)
    ax.set_yticklabels([feature_names[i] for i in indices])
    ax.invert_yaxis()
    ax.set_xlabel('Permutation Importance (mean Â± std)', fontsize=12)
    ax.set_title(f'Top {top_n} Features - Permutation Importance', fontsize=14)
    ax.grid(axis='x', alpha=0.3, linestyle='--')
    
    plt.tight_layout()
    plt.savefig(out_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"[INFO] Permutation Importance å·²ä¿å­˜: {out_path}")


def plot_importance_comparison(rf_model, perm_imp, feature_names, out_path, top_n=15):
    """å¯¹æ¯” RF å†…ç½® importance vs Permutation Importance"""
    rf_importance = rf_model.feature_importances_
    perm_importance = perm_imp.importances_mean
    
    # æ‰¾å‡ºä¸¤è€…çš„ topN å¹¶é›†
    rf_top = set(np.argsort(rf_importance)[::-1][:top_n])
    perm_top = set(np.argsort(perm_importance)[::-1][:top_n])
    top_features = sorted(rf_top | perm_top, 
                         key=lambda x: perm_importance[x], reverse=True)[:top_n]
    
    fig, ax = plt.subplots(figsize=(12, 8))
    
    x = np.arange(len(top_features))
    width = 0.35
    
    # å½’ä¸€åŒ–åˆ°ç›¸åŒèŒƒå›´ä¾¿äºæ¯”è¾ƒ
    rf_norm = rf_importance / rf_importance.max() if rf_importance.max() > 0 else rf_importance
    perm_norm = perm_importance / perm_importance.max() if perm_importance.max() > 0 else perm_importance
    
    ax.barh(x - width/2, [rf_norm[i] for i in top_features], width, 
           label='RF Built-in', color='steelblue', alpha=0.7)
    ax.barh(x + width/2, [perm_norm[i] for i in top_features], width,
           label='Permutation', color='darkorange', alpha=0.7)
    
    ax.set_yticks(x)
    ax.set_yticklabels([feature_names[i] for i in top_features])
    ax.invert_yaxis()
    ax.set_xlabel('Normalized Importance', fontsize=12)
    ax.set_title('Feature Importance Comparison (Normalized)', fontsize=14)
    ax.legend()
    ax.grid(axis='x', alpha=0.3, linestyle='--')
    
    plt.tight_layout()
    plt.savefig(out_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"[INFO] Importance å¯¹æ¯”å›¾å·²ä¿å­˜: {out_path}")


def main():
    parser = argparse.ArgumentParser(description='ä¸¥è°¨ç‰ˆ Random Forest è®­ç»ƒè„šæœ¬')
    
    parser.add_argument('--data', type=str, default=None,
                       help='ç‰¹å¾CSVæ–‡ä»¶è·¯å¾„ï¼ˆç•™ç©ºè‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°ï¼‰')
    parser.add_argument('--cv_splits', type=int, default=5,
                       help='TimeSeriesSplit æŠ˜æ•°')
    parser.add_argument('--n_estimators', type=int, default=300,
                       help='éšæœºæ£®æ—æ ‘çš„æ•°é‡')
    parser.add_argument('--class_weights', type=str, default='balanced_subsample',
                       help='ç±»åˆ«æƒé‡ï¼ˆbalanced_subsample æˆ–é€—å·åˆ†éš”çš„æƒé‡å¦‚ 1,10,15,10ï¼‰')
    parser.add_argument('--out_dir', type=str, default='models',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--vis_dir', type=str, default='datasets/visualizations',
                       help='å¯è§†åŒ–è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # 1. æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
    if args.data is None:
        print("[INFO] æœªæŒ‡å®šæ•°æ®æ–‡ä»¶ï¼Œè‡ªåŠ¨æœç´¢æœ€æ–°æ–‡ä»¶...")
        data_file = find_latest_features_file()
        if data_file is None:
            print("[ERROR] æœªæ‰¾åˆ°ç‰¹å¾æ–‡ä»¶ï¼")
            print("[INFO] è¯·å…ˆè¿è¡Œç‰¹å¾æå–è„šæœ¬")
            return
        print(f"[INFO] æ‰¾åˆ°æœ€æ–°æ–‡ä»¶: {data_file}")
    else:
        data_file = args.data
        if not os.path.exists(data_file):
            print(f"[ERROR] æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
            return
    
    # 2. åŠ è½½æ•°æ®
    print("\n[INFO] åŠ è½½æ•°æ®...")
    df = pd.read_csv(data_file)
    print(f"[INFO] æ•°æ®è¡Œæ•°: {len(df)}")
    print(f"[INFO] åˆ—æ•°: {len(df.columns)}")
    
    # 3. åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
    exclude_cols = ['label', 'time']
    feature_cols = [c for c in df.columns if c not in exclude_cols]
    X = df[feature_cols].values
    y = df['label'].values
    
    print(f"\n[INFO] ç‰¹å¾ç»´åº¦: {X.shape}")
    print(f"[INFO] ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    
    # 4. æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡
    labels = ['èƒŒæ™¯', 'å‡†å¤‡', 'æ ¸å¿ƒ', 'æ¢å¤']
    print(f"\n[INFO] æ ‡ç­¾åˆ†å¸ƒ:")
    for label_idx in range(4):
        count = np.sum(y == label_idx)
        pct = count / len(y) * 100
        print(f"  {labels[label_idx]} ({label_idx}): {count:6d} ({pct:5.2f}%)")
    
    # 5. å¤„ç†ç±»åˆ«æƒé‡
    if args.class_weights == 'balanced_subsample':
        class_weight = 'balanced_subsample'
        print(f"\n[INFO] ä½¿ç”¨è‡ªåŠ¨å¹³è¡¡æƒé‡: balanced_subsample")
    else:
        try:
            weights = [float(w) for w in args.class_weights.split(',')]
            if len(weights) != 4:
                raise ValueError("æƒé‡æ•°é‡å¿…é¡»ä¸º4")
            class_weight = {i: weights[i] for i in range(4)}
            print(f"\n[INFO] ä½¿ç”¨æ‰‹åŠ¨æƒé‡: {class_weight}")
        except Exception as e:
            print(f"[ERROR] æƒé‡æ ¼å¼é”™è¯¯: {e}")
            print("[INFO] ä½¿ç”¨é»˜è®¤: balanced_subsample")
            class_weight = 'balanced_subsample'
    
    # 6. æ„å»º Random Forest æ¨¡å‹
    print(f"\n[INFO] æ„å»º Random Forest æ¨¡å‹...")
    print(f"  - n_estimators: {args.n_estimators}")
    print(f"  - max_depth: None (ä¸é™åˆ¶)")
    print(f"  - min_samples_leaf: 5")
    print(f"  - class_weight: {class_weight}")
    
    rf = RandomForestClassifier(
        n_estimators=args.n_estimators,
        max_depth=None,
        min_samples_leaf=5,
        class_weight=class_weight,
        n_jobs=-1,
        random_state=42
    )
    
    # 7. æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ï¼ˆæ‰‹åŠ¨å®ç°ï¼‰
    print(f"\n[INFO] ä½¿ç”¨ TimeSeriesSplit (n_splits={args.cv_splits}) äº¤å‰éªŒè¯...")
    tscv = TimeSeriesSplit(n_splits=args.cv_splits)
    
    # æ‰‹åŠ¨æ‰§è¡Œäº¤å‰éªŒè¯
    print("[INFO] æ‰§è¡Œäº¤å‰éªŒè¯é¢„æµ‹...")
    y_pred_cv = np.zeros(len(y), dtype=int)
    
    for fold_idx, (train_idx, test_idx) in enumerate(tscv.split(X), 1):
        print(f"  Fold {fold_idx}/{args.cv_splits} - Train: {len(train_idx)}, Test: {len(test_idx)}")
        
        # è®­ç»ƒæ¨¡å‹
        rf_fold = RandomForestClassifier(
            n_estimators=args.n_estimators,
            max_depth=None,
            min_samples_leaf=5,
            class_weight=class_weight,
            n_jobs=-1,
            random_state=42
        )
        rf_fold.fit(X[train_idx], y[train_idx])
        
        # é¢„æµ‹æµ‹è¯•é›†
        y_pred_cv[test_idx] = rf_fold.predict(X[test_idx])
    
    # 8. è¯„ä¼°æŒ‡æ ‡
    print("\n" + "="*60)
    print("äº¤å‰éªŒè¯ç»“æœ")
    print("="*60)
    
    # Classification Report
    report = classification_report(y, y_pred_cv, 
                                   target_names=labels, 
                                   output_dict=True,
                                   zero_division=0)
    
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y, y_pred_cv, target_names=labels, zero_division=0))
    
    # Macro F1
    macro_f1 = report['macro avg']['f1-score']
    print(f"â­ Macro F1-Score: {macro_f1:.4f}")
    
    # Confusion Matrix
    cm = confusion_matrix(y, y_pred_cv)
    print(f"\næ··æ·†çŸ©é˜µ:")
    print(cm)
    
    # 9. è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆç”¨äºç‰¹å¾é‡è¦æ€§å’Œé¢„æµ‹æ¦‚ç‡ï¼‰
    print(f"\n[INFO] è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆå…¨é‡æ•°æ®ï¼‰...")
    rf.fit(X, y)
    
    # 10. Permutation Importance
    print(f"\n[INFO] è®¡ç®— Permutation Importance (n_repeats=10)...")
    perm_imp = permutation_importance(rf, X, y, 
                                      n_repeats=10, 
                                      random_state=42, 
                                      n_jobs=-1)
    
    # 11. è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒï¼ˆä¸º HMM å‡†å¤‡ï¼‰
    print(f"\n[INFO] ç”Ÿæˆæ¯å¸§æ¦‚ç‡åˆ†å¸ƒ...")
    y_proba = rf.predict_proba(X)
    y_pred_final = rf.predict(X)
    
    # åˆ›å»ºæ¦‚ç‡DataFrame
    proba_df = pd.DataFrame(y_proba, columns=[f'prob_{label}' for label in labels])
    proba_df['true_label'] = y
    proba_df['pred_label'] = y_pred_final
    proba_df['is_correct'] = (y == y_pred_final).astype(int)
    
    if 'time' in df.columns:
        proba_df.insert(0, 'time', df['time'].values)
    
    # 12. ä¿å­˜ç»“æœ
    os.makedirs(args.out_dir, exist_ok=True)
    os.makedirs(args.vis_dir, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹
    model_path = os.path.join(args.out_dir, 'rf_rigorous_model.pkl')
    with open(model_path, 'wb') as f:
        pickle.dump(rf, f)
    print(f"\n[INFO] æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    # ä¿å­˜æ¦‚ç‡åˆ†å¸ƒ
    proba_path = os.path.join(args.out_dir, 'rf_frame_proba.csv')
    proba_df.to_csv(proba_path, index=False)
    print(f"[INFO] æ¯å¸§æ¦‚ç‡å·²ä¿å­˜: {proba_path}")
    print(f"  - åŒ…å«åˆ—: {list(proba_df.columns)}")
    
    # ä¿å­˜è®­ç»ƒæŠ¥å‘Š
    report_data = {
        'data_file': data_file,
        'n_samples': int(len(X)),
        'n_features': int(len(feature_cols)),
        'cv_strategy': f'TimeSeriesSplit(n_splits={args.cv_splits})',
        'model_config': {
            'n_estimators': args.n_estimators,
            'max_depth': None,
            'min_samples_leaf': 5,
            'class_weight': str(class_weight)
        },
        'metrics': {
            'accuracy': float(report['accuracy']),
            'macro_f1': float(macro_f1),
            'macro_precision': float(report['macro avg']['precision']),
            'macro_recall': float(report['macro avg']['recall']),
        },
        'per_class_metrics': {
            label: {
                'precision': float(report.get(label, {}).get('precision', 0.0)),
                'recall': float(report.get(label, {}).get('recall', 0.0)),
                'f1-score': float(report.get(label, {}).get('f1-score', 0.0)),
                'support': int(report.get(label, {}).get('support', 0))
            } for label in labels
        },
        'confusion_matrix': cm.tolist(),
        'top_10_features_permutation': [
            {
                'feature': feature_cols[i],
                'importance': float(perm_imp.importances_mean[i]),
                'std': float(perm_imp.importances_std[i])
            }
            for i in np.argsort(perm_imp.importances_mean)[::-1][:10]
        ]
    }
    
    report_path = os.path.join(args.out_dir, 'rf_training_report.json')
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, indent=2, ensure_ascii=False)
    print(f"[INFO] è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # 13. ç”Ÿæˆå¯è§†åŒ–
    print(f"\n[INFO] ç”Ÿæˆå¯è§†åŒ–...")
    
    # æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
    cm_heatmap_path = os.path.join(args.vis_dir, 'rf_confusion_matrix_heatmap.png')
    plot_confusion_matrix_heatmap(y, y_pred_cv, cm_heatmap_path, labels)
    
    # Per-class æ€§èƒ½æŒ‡æ ‡
    per_class_path = os.path.join(args.vis_dir, 'rf_per_class_metrics.png')
    plot_per_class_metrics(report, per_class_path, labels)
    
    # Permutation Importance
    perm_imp_path = os.path.join(args.vis_dir, 'rf_permutation_importance.png')
    plot_permutation_importance(perm_imp, feature_cols, perm_imp_path, top_n=20)
    
    # Importance å¯¹æ¯”
    comparison_path = os.path.join(args.vis_dir, 'rf_importance_comparison.png')
    plot_importance_comparison(rf, perm_imp, feature_cols, comparison_path, top_n=15)
    
    # 14. æ€»ç»“
    print("\n" + "="*60)
    print("è®­ç»ƒå®Œæˆï¼")
    print("="*60)
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"  æ¨¡å‹:     {model_path}")
    print(f"  æ¦‚ç‡:     {proba_path}")
    print(f"  æŠ¥å‘Š:     {report_path}")
    print(f"\nğŸ“Š å¯è§†åŒ–:")
    print(f"  æ··æ·†çŸ©é˜µ: {cm_heatmap_path}")
    print(f"  Per-class: {per_class_path}")
    print(f"  Perm Imp:  {perm_imp_path}")
    print(f"  å¯¹æ¯”å›¾:    {comparison_path}")
    
    print(f"\nğŸ¯ å…³é”®æŒ‡æ ‡:")
    print(f"  Accuracy:  {report['accuracy']:.4f}")
    print(f"  Macro F1:  {macro_f1:.4f}")
    
    # å‡†å¤‡ vs æ¢å¤æ··æ·†åˆ†æ
    prep_to_rec = cm[1, 3]
    rec_to_prep = cm[3, 1]
    prep_total = cm[1].sum()
    rec_total = cm[3].sum()
    
    print(f"\nâš ï¸  å‡†å¤‡ vs æ¢å¤ æ··æ·†åˆ†æ:")
    print(f"  å‡†å¤‡â†’æ¢å¤: {prep_to_rec}/{prep_total} ({prep_to_rec/prep_total*100:.1f}%)")
    print(f"  æ¢å¤â†’å‡†å¤‡: {rec_to_prep}/{rec_total} ({rec_to_prep/rec_total*100:.1f}%)")
    
    print("\nâœ… ä¸‹ä¸€æ­¥å»ºè®®:")
    print("  1. æŸ¥çœ‹æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾ï¼Œåˆ†æå‡†å¤‡/æ¢å¤æ··æ·†æƒ…å†µ")
    print("  2. åŸºäº rf_frame_proba.csv å®ç° Viterbi åºåˆ—å¹³æ»‘")
    print("  3. æ·»åŠ æ—¶é—´ç»“æ„ç‰¹å¾ï¼ˆçª—å£å·®åˆ†ã€æ»‘åŠ¨ç»Ÿè®¡ï¼‰")


if __name__ == '__main__':
    main()
