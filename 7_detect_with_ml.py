#!/usr/bin/env python3
"""
MLæ¨ç†è„šæœ¬ - ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹
"""

import argparse
import os
import pickle
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats as scipy_stats
from scipy.fft import fft

# é…ç½®matplotlibæ”¯æŒä¸­æ–‡
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False


def extract_time_features(window: np.ndarray):
    """æå–æ—¶åŸŸç‰¹å¾ (3è½´) - ä¸è®­ç»ƒè„šæœ¬ä¿æŒä¸€è‡´"""
    features = {}
    
    for axis_idx, axis_name in enumerate(['x', 'y', 'z']):
        axis_data = window[:, axis_idx]
        
        # åŸºæœ¬ç»Ÿè®¡
        features[f'{axis_name}_mean'] = np.mean(axis_data)
        features[f'{axis_name}_std'] = np.std(axis_data)
        features[f'{axis_name}_min'] = np.min(axis_data)
        features[f'{axis_name}_max'] = np.max(axis_data)
        features[f'{axis_name}_ptp'] = np.ptp(axis_data)
        
        # RMS
        features[f'{axis_name}_rms'] = np.sqrt(np.mean(axis_data ** 2))
        
        # ååº¦å’Œå³°åº¦
        features[f'{axis_name}_skew'] = scipy_stats.skew(axis_data)
        features[f'{axis_name}_kurtosis'] = scipy_stats.kurtosis(axis_data)
        
        # è¿‡é›¶ç‡
        zero_crossings = np.sum(np.diff(np.sign(axis_data)) != 0)
        features[f'{axis_name}_zcr'] = zero_crossings / len(axis_data)
    
    # åŠ é€Ÿåº¦å¹…å€¼
    acc_mag = np.linalg.norm(window, axis=1)
    features['mag_mean'] = np.mean(acc_mag)
    features['mag_std'] = np.std(acc_mag)
    features['mag_max'] = np.max(acc_mag)
    features['mag_min'] = np.min(acc_mag)
    
    return features


def extract_freq_features(window: np.ndarray, sample_rate: float = 100.0):
    """æå–é¢‘åŸŸç‰¹å¾ - ä¸è®­ç»ƒè„šæœ¬ä¿æŒä¸€è‡´"""
    features = {}
    
    for axis_idx, axis_name in enumerate(['x', 'y', 'z']):
        axis_data = window[:, axis_idx]
        
        # FFT
        fft_vals = np.abs(fft(axis_data))
        freqs = np.fft.fftfreq(len(axis_data), 1/sample_rate)
        
        # åªå–æ­£é¢‘ç‡éƒ¨åˆ†
        pos_mask = freqs > 0
        fft_vals = fft_vals[pos_mask]
        freqs = freqs[pos_mask]
        
        # ä¸»é¢‘ç‡
        if len(fft_vals) > 0:
            dominant_freq_idx = np.argmax(fft_vals)
            features[f'{axis_name}_dominant_freq'] = freqs[dominant_freq_idx]
            features[f'{axis_name}_dominant_power'] = fft_vals[dominant_freq_idx]
        else:
            features[f'{axis_name}_dominant_freq'] = 0.0
            features[f'{axis_name}_dominant_power'] = 0.0
        
        # é¢‘æ®µèƒ½é‡åˆ†å¸ƒ
        bands = [(0, 2), (2, 5), (5, 10), (10, 50)]
        for low, high in bands:
            band_mask = (freqs >= low) & (freqs < high)
            band_energy = np.sum(fft_vals[band_mask] ** 2)
            features[f'{axis_name}_energy_{low}_{high}Hz'] = band_energy
    
    return features


def extract_custom_features(window: np.ndarray):
    """æå–è‡ªå®šä¹‰ç‰¹å¾ - ä¸è®­ç»ƒè„šæœ¬ä¿æŒä¸€è‡´"""
    features = {}
    
    # ä¸‰è½´æ ‡å‡†å·®æ¯”å€¼
    std_x = np.std(window[:, 0])
    std_y = np.std(window[:, 1])
    std_z = np.std(window[:, 2])
    
    max_std = max(std_x, std_y, std_z)
    if max_std > 0:
        features['std_ratio_x'] = std_x / max_std
        features['std_ratio_y'] = std_y / max_std
        features['std_ratio_z'] = std_z / max_std
    else:
        features['std_ratio_x'] = 0.33
        features['std_ratio_y'] = 0.33
        features['std_ratio_z'] = 0.33
    
    # åŠ é€Ÿåº¦æ¢¯åº¦
    if len(window) > 1:
        gradient = np.diff(window, axis=0)
        features['gradient_mean'] = np.mean(np.linalg.norm(gradient, axis=1))
        features['gradient_max'] = np.max(np.linalg.norm(gradient, axis=1))
    else:
        features['gradient_mean'] = 0.0
        features['gradient_max'] = 0.0
    
    # åŠ¨æ€èŒƒå›´
    acc_mag = np.linalg.norm(window, axis=1)
    features['dynamic_range'] = np.max(acc_mag) - np.min(acc_mag)
    
    return features


def extract_features_from_df(
    df: pd.DataFrame,
    window_size: int = 40,
    stride: int = 1,
    sample_rate: float = 100.0
) -> pd.DataFrame:
    """ä»DataFrameä¸­æå–æ»‘åŠ¨çª—å£ç‰¹å¾"""
    
    acc_cols = ['acc_dyn_x', 'acc_dyn_y', 'acc_dyn_z']
    acc_data = df[acc_cols].values
    time_data = df['time'].values
    
    features_list = []
    window_times = []
    
    print(f"[INFO] å¼€å§‹ç‰¹å¾æå–...")
    print(f"[INFO] çª—å£å¤§å°: {window_size} æ ·æœ¬ ({window_size/sample_rate*1000:.0f}ms)")
    print(f"[INFO] æ­¥é•¿: {stride} æ ·æœ¬ ({stride/sample_rate*1000:.0f}ms)")
    
    total_windows = (len(acc_data) - window_size) // stride + 1
    
    for i in range(0, len(acc_data) - window_size + 1, stride):
        # æå–çª—å£
        window = acc_data[i:i+window_size]
        
        # çª—å£çš„æ—¶é—´ (å–ä¸­å¿ƒç‚¹)
        center_idx = i + window_size // 2
        window_time = time_data[center_idx]
        
        # æå–ç‰¹å¾
        features = {}
        features.update(extract_time_features(window))
        features.update(extract_freq_features(window, sample_rate))
        features.update(extract_custom_features(window))
        
        features_list.append(features)
        window_times.append(window_time)
        
        if (len(features_list) % 5000 == 0):
            print(f"[INFO] å·²å¤„ç† {len(features_list)}/{total_windows} çª—å£...")
    
    print(f"[INFO] ç‰¹å¾æå–å®Œæˆ! æ€»çª—å£æ•°: {len(features_list)}")
    
    # è½¬æ¢ä¸ºDataFrame
    features_df = pd.DataFrame(features_list)
    features_df['time'] = window_times
    
    # æ·»åŠ æ—¶é—´åºåˆ—ç‰¹å¾ (ä¸è®­ç»ƒä¿æŒä¸€è‡´)
    print("[INFO] æ·»åŠ æ—¶é—´åºåˆ—ç‰¹å¾...")
    
    key_cols = ['y_mean', 'y_std', 'y_rms', 'y_ptp', 
                'mag_mean', 'mag_max', 'mag_std',
                'gradient_max', 'dynamic_range']
    
    key_cols = [col for col in key_cols if col in features_df.columns]
    
    # 1. å·®åˆ†ç‰¹å¾
    for col in key_cols:
        features_df[f'{col}_diff'] = features_df[col].diff().fillna(0)
    
    # 2. æ»šåŠ¨ç»Ÿè®¡
    for col in key_cols:
        features_df[f'{col}_roll3_mean'] = features_df[col].rolling(window=3, min_periods=1).mean()
        features_df[f'{col}_roll3_std'] = features_df[col].rolling(window=3, min_periods=1).std().fillna(0)
    
    # 3. åŠ é€Ÿåº¦ç‰¹å¾
    for col in ['y_rms', 'mag_max']:
        if col in features_df.columns:
            features_df[f'{col}_accel'] = features_df[col].diff().diff().fillna(0)
    
    # 4. åŠ¨é‡ç‰¹å¾
    for col in ['y_rms', 'mag_mean']:
        if col in features_df.columns:
            features_df[f'{col}_momentum'] = features_df[col].diff().rolling(window=5, min_periods=1).sum().fillna(0)
    
    # âš ï¸ æ·»åŠ  stroke_id åˆ—ä»¥åŒ¹é…è®­ç»ƒç‰¹å¾ï¼ˆæ¨ç†æ—¶è®¾ä¸º0ï¼‰
    features_df['stroke_id'] = 0
    
    print(f"[INFO] ç‰¹å¾æ€»æ•°: {len(features_df.columns) - 1}  (ä¸å«time)")
    
    return features_df


def visualize_predictions(df_orig: pd.DataFrame, predictions: np.ndarray, 
                         probabilities: np.ndarray, out_dir: str, 
                         window_size: int = 40):
    """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
    print("\n[INFO] ç”Ÿæˆé¢„æµ‹å¯è§†åŒ–...")
    
    # æ ‡ç­¾é…ç½®
    label_names = {0: 'èƒŒæ™¯', 1: 'å‡†å¤‡', 2: 'æ ¸å¿ƒ', 3: 'æ¢å¤', 4: 'è¿‡æ¸¡'}
    colors = {
        0: '#BBDEFB',  # æµ…è“ - èƒŒæ™¯
        1: '#FFF59D',  # é»„è‰² - å‡†å¤‡
        2: '#FF5252',  # çº¢è‰² - æ ¸å¿ƒ
        3: '#90CAF9',  # è“è‰² - æ¢å¤
        4: '#CE93D8'   # ç´«è‰² - è¿‡æ¸¡
    }
    
    # é€‰æ‹©ä¸€æ®µæ•°æ®è¿›è¡Œå¯è§†åŒ– (å‰10ç§’)
    sample_rate = 100.0
    viz_duration = 10.0  # ç§’
    viz_samples = int(viz_duration * sample_rate)
    
    # åˆ›å»ºå¤šä¸ªå¯è§†åŒ–æ ·æœ¬
    n_samples = min(3, len(df_orig) // viz_samples)
    
    for sample_idx in range(n_samples):
        start_idx = sample_idx * viz_samples
        end_idx = start_idx + viz_samples
        
        if end_idx > len(df_orig):
            break
        
        # æå–æ•°æ®æ®µ
        df_segment = df_orig.iloc[start_idx:end_idx]
        
        # å¯¹åº”çš„é¢„æµ‹ç»“æœ (è€ƒè™‘çª—å£ä¸­å¿ƒåç§»)
        pred_start = start_idx
        pred_end = min(end_idx, len(predictions))
        
        if pred_start >= len(predictions):
            continue
        
        preds_segment = predictions[pred_start:pred_end]
        probs_segment = probabilities[pred_start:pred_end]
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(4, 1, figsize=(16, 12), sharex=True)
        
        time_segment = df_segment['time'].values
        pred_time = time_segment[:len(preds_segment)]
        
        # 1. åŸå§‹åŠ é€Ÿåº¦æ•°æ®
        ax = axes[0]
        ax.plot(time_segment, df_segment['acc_dyn_x'], 'r-', alpha=0.6, linewidth=0.8, label='Xè½´')
        ax.plot(time_segment, df_segment['acc_dyn_y'], 'g-', alpha=0.6, linewidth=0.8, label='Yè½´')
        ax.plot(time_segment, df_segment['acc_dyn_z'], 'b-', alpha=0.6, linewidth=0.8, label='Zè½´')
        ax.set_ylabel('åŠ é€Ÿåº¦ (m/sÂ²)', fontsize=11)
        ax.set_title(f'æ ·æœ¬ {sample_idx+1}: åŸå§‹åŠ é€Ÿåº¦æ•°æ®', fontsize=12, fontweight='bold')
        ax.legend(loc='upper right')
        ax.grid(True, alpha=0.3)
        
        # 2. é¢„æµ‹æ ‡ç­¾
        ax = axes[1]
        for label_id in sorted(label_names.keys()):
            mask = preds_segment == label_id
            if np.any(mask):
                ax.scatter(pred_time[mask], preds_segment[mask], 
                          c=colors[label_id], label=label_names[label_id],
                          s=10, alpha=0.7)
        ax.set_ylabel('é¢„æµ‹æ ‡ç­¾', fontsize=11)
        ax.set_ylim([-0.5, 4.5])
        ax.set_yticks(range(5))
        ax.set_yticklabels([label_names[i] for i in range(5)])
        ax.set_title('MLæ¨¡å‹é¢„æµ‹ç»“æœ', fontsize=12, fontweight='bold')
        ax.legend(loc='upper right', ncol=5)
        ax.grid(True, alpha=0.3)
        
        # 3. é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
        ax = axes[2]
        for label_id in range(5):
            ax.plot(pred_time, probs_segment[:, label_id], 
                   color=colors[label_id], label=label_names[label_id],
                   linewidth=1.5, alpha=0.7)
        ax.set_ylabel('æ¦‚ç‡', fontsize=11)
        ax.set_ylim([0, 1])
        ax.set_title('å„é˜¶æ®µé¢„æµ‹æ¦‚ç‡', fontsize=12, fontweight='bold')
        ax.legend(loc='upper right', ncol=5)
        ax.grid(True, alpha=0.3)
        
        # 4. é¢„æµ‹ç½®ä¿¡åº¦ (æœ€å¤§æ¦‚ç‡)
        ax = axes[3]
        max_probs = np.max(probs_segment, axis=1)
        ax.plot(pred_time, max_probs, 'k-', linewidth=1.5, label='ç½®ä¿¡åº¦')
        ax.axhline(y=0.5, color='r', linestyle='--', linewidth=1, alpha=0.5, label='0.5é˜ˆå€¼')
        ax.fill_between(pred_time, 0, max_probs, alpha=0.3, color='gray')
        ax.set_ylabel('ç½®ä¿¡åº¦', fontsize=11)
        ax.set_xlabel('æ—¶é—´ (s)', fontsize=11)
        ax.set_ylim([0, 1])
        ax.set_title('é¢„æµ‹ç½®ä¿¡åº¦', fontsize=12, fontweight='bold')
        ax.legend(loc='upper right')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        out_path = os.path.join(out_dir, f'prediction_sample_{sample_idx+1}.png')
        plt.savefig(out_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"  âœ“ ä¿å­˜å¯è§†åŒ–: prediction_sample_{sample_idx+1}.png")
    
    # ç”Ÿæˆç»Ÿè®¡å›¾
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # é¢„æµ‹æ ‡ç­¾åˆ†å¸ƒ
    ax = axes[0]
    label_counts = pd.Series(predictions).value_counts().sort_index()
    bars = ax.bar([label_names[i] for i in label_counts.index], 
                   label_counts.values,
                   color=[colors[i] for i in label_counts.index],
                   alpha=0.7)
    ax.set_ylabel('è®¡æ•°', fontsize=11)
    ax.set_title('é¢„æµ‹æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')
    
    # æ·»åŠ ç™¾åˆ†æ¯”æ ‡ç­¾
    total = len(predictions)
    for bar, count in zip(bars, label_counts.values):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
               f'{count}\n({count/total*100:.1f}%)',
               ha='center', va='bottom', fontsize=9)
    
    # å¹³å‡ç½®ä¿¡åº¦
    ax = axes[1]
    mean_confidence = []
    for label_id in range(5):
        mask = predictions == label_id
        if np.any(mask):
            mean_conf = np.mean(np.max(probabilities[mask], axis=1))
            mean_confidence.append(mean_conf)
        else:
            mean_confidence.append(0)
    
    bars = ax.bar([label_names[i] for i in range(5)], mean_confidence,
                   color=[colors[i] for i in range(5)], alpha=0.7)
    ax.set_ylabel('å¹³å‡ç½®ä¿¡åº¦', fontsize=11)
    ax.set_ylim([0, 1])
    ax.set_title('å„é˜¶æ®µå¹³å‡é¢„æµ‹ç½®ä¿¡åº¦', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, conf in zip(bars, mean_confidence):
        if conf > 0:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{conf:.3f}',
                   ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    stats_path = os.path.join(out_dir, 'prediction_statistics.png')
    plt.savefig(stats_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"  âœ“ ä¿å­˜ç»Ÿè®¡å›¾: prediction_statistics.png")


def main():
    parser = argparse.ArgumentParser(description='MLæ¨ç†è„šæœ¬')
    
    parser.add_argument('--data', type=str, required=True,
                       help='æ–°æ•°æ®CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model', type=str, default='models/rf_rigorous_model.pkl',
                       help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--window_size', type=int, default=40,
                       help='çª—å£å¤§å°(æ ·æœ¬æ•°)')
    parser.add_argument('--stride', type=int, default=1,
                       help='æ»‘åŠ¨æ­¥é•¿(æ ·æœ¬æ•°)')
    parser.add_argument('--sample_rate', type=float, default=100.0,
                       help='é‡‡æ ·ç‡(Hz)')
    parser.add_argument('--out_dir', type=str, default='detection_comparison',
                       help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # 1. æ£€æŸ¥æ–‡ä»¶
    if not os.path.exists(args.data):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data}")
    
    if not os.path.exists(args.model):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model}")
    
    # 2. åŠ è½½æ¨¡å‹
    print(f"\n[INFO] åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹: {args.model}")
    with open(args.model, 'rb') as f:
        model = pickle.load(f)
    print(f"[INFO] æ¨¡å‹åŠ è½½æˆåŠŸ!")
    print(f"  - æ¨¡å‹ç±»å‹: {type(model).__name__}")
    print(f"  - ç‰¹å¾æ•°é‡: {model.n_features_in_}")
    
    # 3. åŠ è½½æ–°æ•°æ®
    print(f"\n[INFO] åŠ è½½æ–°æ•°æ®: {args.data}")
    df = pd.read_csv(args.data)
    print(f"[INFO] æ•°æ®è¡Œæ•°: {len(df)}")
    print(f"[INFO] æ•°æ®åˆ—: {list(df.columns)}")
    
    # æ£€æŸ¥å¿…è¦çš„åˆ—
    required_cols = ['time', 'acc_dyn_x', 'acc_dyn_y', 'acc_dyn_z']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"æ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_cols}")
    
    # 4. æå–ç‰¹å¾
    print(f"\n[INFO] æå–ç‰¹å¾...")
    features_df = extract_features_from_df(
        df,
        window_size=args.window_size,
        stride=args.stride,
        sample_rate=args.sample_rate
    )
    
    # 5. å‡†å¤‡ç‰¹å¾çŸ©é˜µ (æ’é™¤timeåˆ—)
    feature_cols = [col for col in features_df.columns if col != 'time']
    X = features_df[feature_cols].values
    
    print(f"\n[INFO] ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
    print(f"[INFO] æ¨¡å‹æœŸæœ›ç‰¹å¾æ•°: {model.n_features_in_}")
    
    if X.shape[1] != model.n_features_in_:
        raise ValueError(f"ç‰¹å¾æ•°é‡ä¸åŒ¹é…! æå–äº†{X.shape[1]}ä¸ªç‰¹å¾ï¼Œä½†æ¨¡å‹éœ€è¦{model.n_features_in_}ä¸ª")
    
    # 6. è¿›è¡Œé¢„æµ‹
    print(f"\n[INFO] å¼€å§‹é¢„æµ‹...")
    predictions = model.predict(X)
    probabilities = model.predict_proba(X)
    
    # 7. ä¿å­˜ç»“æœ
    os.makedirs(args.out_dir, exist_ok=True)
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    results_df = features_df.copy()
    results_df['predicted_label'] = predictions
    for i in range(5):
        results_df[f'prob_label_{i}'] = probabilities[:, i]
    results_df['max_probability'] = np.max(probabilities, axis=1)
    
    base_name = os.path.splitext(os.path.basename(args.data))[0]
    results_path = os.path.join(args.out_dir, f'{base_name}_predictions.csv')
    results_df.to_csv(results_path, index=False)
    print(f"\n[INFO] é¢„æµ‹ç»“æœå·²ä¿å­˜: {results_path}")
    
    # 8. ç»Ÿè®¡åˆ†æ
    print(f"\n{'='*60}")
    print("é¢„æµ‹ç»“æœç»Ÿè®¡")
    print('='*60)
    
    label_names = {0: 'èƒŒæ™¯', 1: 'å‡†å¤‡', 2: 'æ ¸å¿ƒ', 3: 'æ¢å¤', 4: 'è¿‡æ¸¡'}
    
    print("\næ ‡ç­¾åˆ†å¸ƒ:")
    label_counts = pd.Series(predictions).value_counts().sort_index()
    total = len(predictions)
    for label, count in label_counts.items():
        pct = count / total * 100
        print(f"  {label_names.get(label, str(label))} ({label}): {count:6d} ({pct:5.2f}%)")
    
    print("\nå„é˜¶æ®µå¹³å‡ç½®ä¿¡åº¦:")
    for label_id in range(5):
        mask = predictions == label_id
        if np.any(mask):
            mean_conf = np.mean(np.max(probabilities[mask], axis=1))
            print(f"  {label_names[label_id]}: {mean_conf:.4f}")
    
    overall_conf = np.mean(np.max(probabilities, axis=1))
    print(f"\næ•´ä½“å¹³å‡ç½®ä¿¡åº¦: {overall_conf:.4f}")
    
    # 9. å¯è§†åŒ–
    print(f"\n[INFO] ç”Ÿæˆå¯è§†åŒ–...")
    visualize_predictions(df, predictions, probabilities, args.out_dir, args.window_size)
    
    print(f"\n{'='*60}")
    print("æ¨ç†å®Œæˆ!")
    print('='*60)
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"  é¢„æµ‹ç»“æœ: {results_path}")
    print(f"  å¯è§†åŒ–ç›®å½•: {args.out_dir}")
    print(f"\nâœ… ä¸‹ä¸€æ­¥å»ºè®®:")
    print("  1. æŸ¥çœ‹å¯è§†åŒ–å›¾ç‰‡ï¼ŒéªŒè¯é¢„æµ‹æ•ˆæœ")
    print("  2. å¦‚æœæ•ˆæœä¸ç†æƒ³ï¼Œå¯ä»¥è€ƒè™‘:")
    print("     - æ£€æŸ¥æ•°æ®è´¨é‡")
    print("     - è°ƒæ•´çª—å£å¤§å°å’Œæ­¥é•¿")
    print("     - é‡æ–°è®­ç»ƒæ¨¡å‹")


if __name__ == '__main__':
    main()
